{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "249a4c27-7c8c-41ef-b479-0e3f26558e11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Silver Transformations - Phase 1 Data Load logics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b6a703f-1ef2-49e8-8eb4-d8a75c334257",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, upper, current_timestamp\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    TimestampType, StringType, DoubleType\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72000890-ed88-490b-be6f-78a94fc00691",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG ppe;\n",
    "USE SCHEMA silver;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee06e255-6dca-4a15-a49e-b57fb8ed9e15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "last_ts = spark.sql(\n",
    "    \"SELECT last_processed_ingestion_ts FROM silver_processing_watermark\"\n",
    ").collect()\n",
    "\n",
    "if last_ts:\n",
    "    last_ts = last_ts[0][0]\n",
    "    print(\"Last processed Bronze ingestion_ts:\", last_ts)\n",
    "else:\n",
    "    print(\"No last_processed_ingestion_ts found in silver_processing_watermark.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "207be7d2-6784-4a76-ac23-378fe1e0af05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read ONLY New Bronze Records\n",
    "if last_ts:\n",
    "    bronze_df = spark.sql(\n",
    "        f\"\"\"\n",
    "        SELECT file_name, raw_text, ingestion_ts\n",
    "        FROM ppe.bronze.bronze_raw_transactions\n",
    "        WHERE ingestion_ts > timestamp('{last_ts}')\n",
    "        \"\"\"\n",
    "    )\n",
    "else:\n",
    "    bronze_df = spark.sql(\n",
    "        \"\"\"\n",
    "        SELECT file_name, raw_text, ingestion_ts\n",
    "        FROM ppe.bronze.bronze_raw_transactions\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "if bronze_df.count() == 0:\n",
    "    print(\"No new Bronze data to process.\")\n",
    "    dbutils.notebook.exit(\"Silver up-to-date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22c59b3c-a9b9-4d87-a69b-f6d206b83ac1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    TimestampType, StringType, DoubleType\n",
    ")\n",
    "\n",
    "silver_schema = StructType([\n",
    "    StructField(\"transaction_time\", TimestampType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"merchant\", StringType(), True),\n",
    "    StructField(\"transaction_id\", StringType(), True),\n",
    "    StructField(\"utr\", StringType(), True),\n",
    "    StructField(\"paid_by\", StringType(), True),\n",
    "    StructField(\"ingestion_ts\", TimestampType(), True),\n",
    "    StructField(\"dq_status\", StringType(), True),\n",
    "    StructField(\"dq_reason\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True)   \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f32fa2b0-e45a-4ee7-96fa-9190f5b58860",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Load Bronze\n",
    "bronze_df = spark.sql(\"SELECT * FROM ppe.bronze.bronze_raw\")\n",
    "raw_text = bronze_df.collect()[0][\"raw_text\"]\n",
    "\n",
    "# Split lines and clean\n",
    "lines = [l.strip() for l in raw_text.split(\"\\n\") if l.strip()]\n",
    "\n",
    "# Helper: ignore noise\n",
    "def is_noise(line):\n",
    "    return (\n",
    "        line.startswith(\"Page \")\n",
    "        or \"system generated statement\" in line.lower()\n",
    "        or \"transaction statement for\" in line.lower()\n",
    "        or \"date transaction details type amount\" in line.lower()\n",
    "        or re.match(r\"\\d{2}\\s[A-Za-z]{3},\\s\\d{4}\\s-\\s\\d{2}\\s[A-Za-z]{3},\\s\\d{4}\", line)\n",
    "    )\n",
    "\n",
    "clean_lines = [l for l in lines if not is_noise(l)]\n",
    "\n",
    "transactions = []\n",
    "i = 0\n",
    "n = len(clean_lines)\n",
    "\n",
    "while i < n:\n",
    "\n",
    "    # 1️⃣ Transaction starts with a date\n",
    "    if re.match(r\"^[A-Za-z]{3}\\s+\\d{2},\\s+\\d{4}\", clean_lines[i]):\n",
    "\n",
    "        block = []\n",
    "        j = i\n",
    "\n",
    "        # 2️⃣ Collect lines until Paid by / Credited to\n",
    "        while j < n:\n",
    "            block.append(clean_lines[j])\n",
    "            if clean_lines[j].startswith(\"Paid by\") or clean_lines[j].startswith(\"Credited to\"):\n",
    "                break\n",
    "            j += 1\n",
    "\n",
    "        block_text = \" \".join(block)\n",
    "\n",
    "        try:\n",
    "            # Date\n",
    "            date_str = re.search(r\"[A-Za-z]{3}\\s+\\d{2},\\s+\\d{4}\", block[0]).group()\n",
    "\n",
    "            # Time (always second line)\n",
    "            time_match = re.search(r\"\\d{2}:\\d{2}\\s*(am|pm)\", block[1], re.I)\n",
    "            txn_time = datetime.strptime(\n",
    "                f\"{date_str} {time_match.group()}\",\n",
    "                \"%b %d, %Y %I:%M %p\"\n",
    "            )\n",
    "\n",
    "            # Amount\n",
    "            amount = float(\n",
    "                re.search(r\"₹([\\d,]+(\\.\\d+)?)\", block_text)\n",
    "                .group(1)\n",
    "                .replace(\",\", \"\")\n",
    "            )\n",
    "\n",
    "            # Type\n",
    "            txn_type = \"DEBIT\" if \"DEBIT\" in block_text else \"CREDIT\"\n",
    "\n",
    "            # Merchant\n",
    "            merchant_match = re.search(\n",
    "                r\"(Paid to|Payment to|Transfer to|Refund from)\\s+(.*?)\\s+(DEBIT|CREDIT)\",\n",
    "                block_text,\n",
    "                re.I\n",
    "            )\n",
    "            merchant = merchant_match.group(2).strip() if merchant_match else None\n",
    "\n",
    "            # Transaction ID\n",
    "            txn_id_match = re.search(r\"Transaction ID\\s+([A-Z0-9]+)\", block_text)\n",
    "            transaction_id = txn_id_match.group(1) if txn_id_match else None\n",
    "\n",
    "            # UTR\n",
    "            utr_match = re.search(r\"UTR No\\.?\\s*([0-9]+)\", block_text)\n",
    "            utr = utr_match.group(1) if utr_match else None\n",
    "\n",
    "            # Paid by → Bank\n",
    "            paid_by_match = re.search(r\"(Paid by|Credited to)\\s*([X0-9]+)\", block_text)\n",
    "            mask = paid_by_match.group(2) if paid_by_match else None\n",
    "\n",
    "            if mask == \"XXXXXX7267\":\n",
    "                paid_by = \"SBI\"\n",
    "            elif mask == \"XXXX438045\":\n",
    "                paid_by = \"AXIS\"\n",
    "            else:\n",
    "                paid_by = \"UNKNOWN\"\n",
    "\n",
    "            transactions.append(Row(\n",
    "                transaction_time=txn_time,\n",
    "                amount=amount,\n",
    "                type=txn_type,\n",
    "                merchant=merchant,\n",
    "                transaction_id=transaction_id,\n",
    "                utr=utr,\n",
    "                paid_by=paid_by,\n",
    "                ingestion_ts=datetime.now(),\n",
    "                dq_status=\"VALID\",\n",
    "                dq_reason=None,\n",
    "                category=None\n",
    "            ))\n",
    "\n",
    "        except Exception as e:\n",
    "            transactions.append(Row(\n",
    "                transaction_time=None,\n",
    "                amount=None,\n",
    "                type=None,\n",
    "                merchant=None,\n",
    "                transaction_id=None,\n",
    "                utr=None,\n",
    "                paid_by=None,\n",
    "                ingestion_ts=datetime.now(),\n",
    "                dq_status=\"INVALID\",\n",
    "                dq_reason=str(e),\n",
    "                category=None\n",
    "            ))\n",
    "\n",
    "        i = j + 1  # move to next transaction\n",
    "\n",
    "    else:\n",
    "        i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f6d1bc1-5b14-415c-8fa1-b8aa98bf4d97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create Data frame and Duplication\n",
    "silver_df = spark.createDataFrame(transactions, silver_schema)\n",
    "silver_dedup = silver_df.dropDuplicates([\"transaction_id\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d42585e2-3992-49b1-bfae-b4d629160ce3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Silver Enhancement - Phase 2\n",
    "Data Quality rules + Deduplication**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05d99b4f-db50-4d22-96fb-f5cd674622f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, lit\n",
    "\n",
    "silver_df_dq = (\n",
    "    silver_df\n",
    "    .withColumn(\n",
    "        \"dq_status\",\n",
    "        when(\n",
    "            (silver_df.transaction_time.isNull()) |\n",
    "            (silver_df.amount.isNull()) |\n",
    "            (silver_df.amount <= 0) |\n",
    "            (silver_df.transaction_id.isNull()) |\n",
    "            (silver_df.merchant.isNull()) |\n",
    "            (silver_df.utr.isNull()),\n",
    "            lit(\"INVALID\")\n",
    "        ).otherwise(lit(\"VALID\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"dq_reason\",\n",
    "        when(silver_df.transaction_time.isNull(), lit(\"Missing transaction_time\"))\n",
    "        .when(silver_df.amount.isNull(), lit(\"Missing amount\"))\n",
    "        .when(silver_df.amount <= 0, lit(\"Invalid amount\"))\n",
    "        .when(silver_df.transaction_id.isNull(), lit(\"Missing transaction_id\"))\n",
    "        .when(silver_df.merchant.isNull(), lit(\"Missing Merchant\"))\n",
    "        .when(silver_df.utr.isNull(), lit(\"Missing utr\"))\n",
    "        .otherwise(lit(None))\n",
    "    )\n",
    ")\n",
    "\n",
    "display(silver_df_dq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d36f61a5-c32d-4373-a1d0-fc99cb4c03eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply Categorization\n",
    "silver_enriched = (\n",
    "    silver_df_dq\n",
    "    .withColumn(\n",
    "        \"category\",\n",
    "        when(col(\"dq_status\") != \"VALID\", \"UNKNOWN\")\n",
    "\n",
    "        # Shares / Investments\n",
    "        .when(upper(col(\"merchant\")).rlike(\"ANGEL|ZERODHA|UPSTOX|GROWW|PAYTM MONEY|COIN|MUTUAL\"),\n",
    "              \"Shares_Investments\")\n",
    "\n",
    "        # Medical\n",
    "        .when(upper(col(\"merchant\")).rlike(\"PHARMACY|MEDICAL|HOSPITAL\"),\n",
    "              \"Medical\")\n",
    "\n",
    "        # Food\n",
    "        .when(upper(col(\"merchant\")).rlike(\n",
    "            \"CAFE|CURRY|CURRIES|COOLDRIK|COOLDRINK|BURGER|FOOD|HOTEL|RESTAURANT|TIFFIN|UDUPI\"),\n",
    "              \"Food\")\n",
    "\n",
    "        # Shopping\n",
    "        .when(upper(col(\"merchant\")).rlike(\"TRENDS|FASHION|CLOTH|SHOPPING\"),\n",
    "              \"Shopping\")\n",
    "\n",
    "        # Travel\n",
    "        .when(upper(col(\"merchant\")).rlike(\"RAIL|IRCTC|BUS|METRO|OLA|UBER\"),\n",
    "              \"Travel\")\n",
    "\n",
    "        # Fuel\n",
    "        .when(upper(col(\"merchant\")).rlike(\"PETROL|FILLING|FUEL|STATION\"),\n",
    "              \"Fuel\")\n",
    "\n",
    "        # Groceries\n",
    "        .when(upper(col(\"merchant\")).rlike(\"KIRANA|STORE|MART|GENERAL\"),\n",
    "              \"Groceries\")\n",
    "\n",
    "        # Utilities\n",
    "        .when(upper(col(\"merchant\")).rlike(\"RECHARGE|ELECTRICITY|JIO|AIRTEL\"),\n",
    "              \"Utilities\")\n",
    "\n",
    "        # Rent\n",
    "        .when(upper(col(\"merchant\")).rlike(\"OWNER|RENT\"),\n",
    "              \"Rent\")\n",
    "\n",
    "        .otherwise(\"Other\")\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd557679-93f8-4361-b292-0e0cf26c7fe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final Silver Table\n",
    "silver_final = (\n",
    "    silver_enriched\n",
    "    .withColumnRenamed(\"bronze_ingestion_ts\", \"ingestion_ts\")\n",
    "    .withColumn(\"ingestion_ts\", current_timestamp())\n",
    "    .select(\n",
    "        \"transaction_time\",\n",
    "        \"amount\",\n",
    "        \"type\",\n",
    "        \"merchant\",\n",
    "        \"transaction_id\",\n",
    "        \"utr\",\n",
    "        \"paid_by\",\n",
    "        \"category\",\n",
    "        \"dq_status\",\n",
    "        \"dq_reason\",\n",
    "        \"ingestion_ts\"\n",
    "    )\n",
    ")\n",
    "\n",
    "(\n",
    "    silver_final\n",
    "    .write.mode(\"append\")\n",
    "    .saveAsTable(\"ppe.silver.silver_transactions\")\n",
    ")\n",
    "\n",
    "display(silver_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "919331cc-3110-41a9-852f-116aabb565d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "max_ts = (\n",
    "    bronze_df\n",
    "    .selectExpr(\"max(ingestion_ts) as max_ts\")\n",
    "    .collect()[0][\"max_ts\"]\n",
    ")\n",
    "\n",
    "if max_ts is not None:\n",
    "    spark.sql(f\"\"\"\n",
    "        UPDATE silver_processing_watermark\n",
    "        SET last_processed_ingestion_ts = timestamp('{max_ts}')\n",
    "    \"\"\")\n",
    "    print(\"Silver watermark updated to:\", max_ts)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5265272586335033,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Silver_Layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
